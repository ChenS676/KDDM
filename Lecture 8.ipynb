{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第五课 图上的其他深度学习模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面的课程中我们介绍了许多图神经网络模型。除了图神经网络，针对于图数据的深度学习模型还有很多，比如图上的自编码器、变分自编码器、循环神经网络和对抗生成网络等。在这一课中，我们对自编码器和变分自编码器进行代码实践。这其中包括了对模型细节和它们的应用的讲解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 链接预测数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链接预测（link prediction）是常见的与图有关的任务。该任务旨在预测两个节点之间是否存在链接（link），即是否存在边。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于链接预测的数据集，我们可以从节点分类任务的数据集直接构造。比如我们之前常用的Cora数据集，就可以无视掉它的节点标签，把Cora图里面的边当成训练/测试数据。下面我们具体来实践一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 构造一个transform，用于对数据的预处理\n",
    "transform = T.Compose([\n",
    "    T.NormalizeFeatures(),  # 对特征进行标准化\n",
    "    T.ToDevice(device),    # 把数据放到cpu或者gpu上\n",
    "    T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,  # 这一步很关键，是在构造链接预测的数据集\n",
    "                      split_labels=True, add_negative_train_samples=False),])\n",
    "\n",
    "\n",
    "dataset = Planetoid('./', name='Cora', transform=transform)\n",
    "train_data, val_data, test_data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们来看一下具体的数据长什么样：\n",
    "* 我们不需要关注y, train_mask, val_mask, test_mask；这些是节点分类里需要用到的信息。\n",
    "* pos_edge_label_index是正边样本的索引，pos_edge_label是其标签（全为1）\n",
    "* neg_edge_label_index是负边样本的索引，neg_edge_label是其标签（全为0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], pos_edge_label=[4488], pos_edge_label_index=[2, 4488])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       " tensor([[2450,  279,  836,  ..., 2510,  787,  538],\n",
       "         [2113,  304, 2403,  ..., 1899,   55, 1286]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.pos_edge_label, train_data.pos_edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], pos_edge_label=[263], pos_edge_label_index=[2, 263], neg_edge_label=[263], neg_edge_label_index=[2, 263])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.neg_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 9502], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], pos_edge_label=[527], pos_edge_label_index=[2, 527], neg_edge_label=[527], neg_edge_label_index=[2, 527])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得注意的是：\n",
    "* train_data中没有自带负边样本neg_edge_label_index，因为我们会在训练过程中自己采样负样本。\n",
    "* train_data和val_data里面的图是一样的（edge_index是一样的），但是他们的pos_edge_label_index（正边样本）和neg_edge_label_index（负边样本）不一样。可以看到train_data中有4488个正边样本，而val_data中只有263个正边样本（二者比例是85:5）。\n",
    "* test_data中的图和train_data的图不一样了。可以看到test_data中的edge_index要多一些（多527个），527也就是test_data中的正边样本数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对于图数据的自编码器我们称之为GAE (Graph AutoEncoder)。其包含两个组成部分，编码器（encoder）和解码器（decoder）。图上的编码器常用的就是GCN了；而解码器呢通常用一个内积来表示。具体地，给定两个节点的节点表示，解码器将计算二者的内积，其结果作为两个节点之间存在边的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先构造编码器，由两层GCN组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    \"\"\"GCN组成的编码器\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后构建解码器，将给定的节点对映射到[0，1]之间，以表示边存在的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InnerProductDecoder(torch.nn.Module):\n",
    "    \"\"\"解码器，用向量内积表示重建的图结构\"\"\"\n",
    "    \n",
    "    def forward(self, z, edge_index, sigmoid=True):\n",
    "        \"\"\"\n",
    "        参数说明：\n",
    "        z: 节点表示\n",
    "        edge_index: 边索引，也就是节点对\n",
    "        \"\"\"\n",
    "        value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=1)\n",
    "        return torch.sigmoid(value) if sigmoid else value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAE(torch.nn.Module):\n",
    "    \"\"\"图自编码器。\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = InnerProductDecoder()\n",
    "\n",
    "    def encode(self, *args, **kwargs): \n",
    "        \"\"\"编码功能\"\"\"\n",
    "        return self.encoder(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        \"\"\"解码功能\"\"\"\n",
    "        return self.decoder(*args, **kwargs)\n",
    "\n",
    "    def recon_loss(self, z, pos_edge_index, neg_edge_index=None):\n",
    "        \"\"\"计算正边和负边的二值交叉熵\n",
    "        \n",
    "        参数说明\n",
    "        ----\n",
    "        z: 编码器的输出\n",
    "        pos_edge_index: 正边的边索引\n",
    "        neg_edge_index: 负边的边索引\n",
    "        \"\"\"\n",
    "        EPS = 1e-15 # EPS是一个很小的值，防止取对数的时候出现0值\n",
    "\n",
    "        pos_loss = -torch.log(\n",
    "            self.decoder(z, pos_edge_index) + EPS).mean() # 正样本的损失函数\n",
    "\n",
    "        if neg_edge_index is None:\n",
    "            neg_edge_index = torch_geometric.utils.negative_sampling(pos_edge_index, z.size(0)) # 负采样\n",
    "        neg_loss = -torch.log(\n",
    "            1 - self.decoder(z, neg_edge_index) + EPS).mean() # 负样本的损失函数\n",
    "\n",
    "        return pos_loss + neg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels, out_channels = dataset.num_features, 16\n",
    "model = GAE(GCNEncoder(in_channels, out_channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.7152e-03,  2.9030e-03,  1.5190e-03,  ..., -1.2694e-03,\n",
       "          -1.2237e-03,  2.0293e-03],\n",
       "         [ 1.1495e-03,  1.6908e-03,  1.7738e-03,  ...,  6.4568e-05,\n",
       "           3.8351e-04, -1.9860e-03],\n",
       "         [ 1.6197e-03,  2.4107e-03, -3.1520e-04,  ..., -1.1129e-03,\n",
       "           2.1450e-05,  1.8590e-03],\n",
       "         ...,\n",
       "         [ 8.3341e-03, -5.4902e-03,  4.5780e-03,  ..., -5.8314e-03,\n",
       "          -1.0872e-02,  1.1104e-03],\n",
       "         [ 1.7777e-03,  3.2905e-03, -1.7124e-03,  ..., -1.5974e-03,\n",
       "          -2.0170e-03,  7.0474e-04],\n",
       "         [ 2.7947e-03,  3.9654e-03, -2.5049e-03,  ..., -2.2268e-03,\n",
       "          -1.0494e-03, -8.5848e-04]], grad_fn=<AddBackward0>),\n",
       " torch.Size([2708, 16]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = model.encode(train_data.x, train_data.edge_index)\n",
    "latent, latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.5000,  ..., 0.5001, 0.5000, 0.5001],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(latent, train_data.edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 变分自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变分自编码器和自编码器基本结构相同，都是一个编码器加一个解码器。它们的主要区别是，变分自编码器编码后的隐层表示不再是连续的向量表示，而是通过一个高斯分布来表示。具体地，变分自编码器学习的是这个高斯分布的均值（下面用变量`mu`来表示）和标准差（下面用变量`std`来表示）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LOGSTD = 10\n",
    "\n",
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels)\n",
    "        self.conv_mu = GCNConv(2 * out_channels, out_channels)\n",
    "        self.conv_logstd = GCNConv(2 * out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)\n",
    "    \n",
    "class VGAE(GAE): \n",
    "    \"\"\"变分自编码器。继承自GAE这个类，可以使用GAE里面定义的函数。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder=None):\n",
    "        super().__init__(encoder, decoder)\n",
    "\n",
    "    def reparametrize(self, mu, logstd):\n",
    "        if self.training:\n",
    "            return mu + torch.randn_like(logstd) * torch.exp(logstd)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        \"\"\"编码功能\"\"\"\n",
    "        self.__mu__, self.__logstd__ = self.encoder(*args, **kwargs) # 编码后的mu和std表示一个分布\n",
    "        self.__logstd__ = self.__logstd__.clamp(max=MAX_LOGSTD) # 这里把std最大值限制一下\n",
    "        z = self.reparametrize(self.__mu__, self.__logstd__) # 进行reparametrization，这样才能够训练模型\n",
    "        return z\n",
    "\n",
    "    def kl_loss(self, mu=None, logstd=None):\n",
    "        \"\"\"我们给隐变量的分布加上（0，I）高斯变量的先验，即希望隐变量分布服从（0，I）的高斯分布\n",
    "        这两个分布的差别用KL损失来衡量。\"\"\"\n",
    "        mu = self.__mu__ if mu is None else mu\n",
    "        logstd = self.__logstd__ if logstd is None else logstd.clamp(\n",
    "            max=MAX_LOGSTD)\n",
    "        return -0.5 * torch.mean(\n",
    "            torch.sum(1 + 2 * logstd - mu**2 - logstd.exp()**2, dim=1)) # 两个高斯分布之间的KL损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（两个高斯分布的kl loss的公式可以参考该[链接](https://stats.stackexchange.com/questions/234757/how-to-use-kullback-leibler-divergence-if-mean-and-standard-deviation-of-of-two)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6204, -1.5778, -0.3021,  ...,  0.3272, -0.8982,  0.7481],\n",
       "         [ 0.3182,  0.6585, -1.6189,  ...,  0.7478,  0.3599,  0.2559],\n",
       "         [ 0.3998, -2.1663,  1.6750,  ..., -1.6075,  0.5851,  1.0506],\n",
       "         ...,\n",
       "         [ 0.2672, -0.5493, -0.8153,  ...,  0.9819,  0.4806, -0.8723],\n",
       "         [ 1.5876,  0.1912, -0.0117,  ..., -1.2229,  0.3848, -0.7511],\n",
       "         [ 0.5664, -0.3713,  0.7488,  ..., -0.7509,  0.4346, -1.9481]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " torch.Size([2708, 16]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = model.encode(train_data.x, train_data.edge_index)\n",
    "latent, latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1584, 0.0016, 0.0113,  ..., 0.9907, 0.2118, 0.8493],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(latent, train_data.edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 训练自编码器和变分自编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们展示自编码器和变分自编码器的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gae(model):\n",
    "    \"\"\"训练GAE模型\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train_vgae(model):\n",
    "    \"\"\"训练VGAE模型，损失函数由重建损失和kl损失组成\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(train_data.x, train_data.edge_index)\n",
    "    loss = model.recon_loss(z, train_data.pos_edge_label_index)\n",
    "    loss = loss + (1 / train_data.num_nodes) * model.kl_loss() # 加上kl loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    \"\"\"测试模型\"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    model.eval()\n",
    "    pos_edge_index = data.pos_edge_label_index\n",
    "    neg_edge_index = data.neg_edge_label_index\n",
    "    \n",
    "    z = model.encode(data.x, data.edge_index)\n",
    "    pos_y = z.new_ones(pos_edge_index.size(1)) # 正样本标签\n",
    "    neg_y = z.new_zeros(neg_edge_index.size(1)) # 负样本标签\n",
    "    y = torch.cat([pos_y, neg_y], dim=0)\n",
    "\n",
    "    pos_pred = model.decoder(z, pos_edge_index)\n",
    "    neg_pred = model.decoder(z, neg_edge_index) \n",
    "    pred = torch.cat([pos_pred, neg_pred], dim=0)\n",
    "\n",
    "    y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\n",
    "\n",
    "    return roc_auc_score(y, pred), average_precision_score(y, pred) # 计算AUC和AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练GAE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss_train: 0.9227, AUC: 0.8880, AP: 0.8941\n",
      "Epoch: 200, Loss_train: 0.8615, AUC: 0.9138, AP: 0.9231\n",
      "Epoch: 300, Loss_train: 0.8566, AUC: 0.9201, AP: 0.9287\n",
      "Epoch: 400, Loss_train: 0.8397, AUC: 0.9229, AP: 0.9325\n",
      "Epoch: 500, Loss_train: 0.8227, AUC: 0.9216, AP: 0.9327\n",
      "Epoch: 600, Loss_train: 0.8273, AUC: 0.9210, AP: 0.9331\n",
      "Epoch: 700, Loss_train: 0.8080, AUC: 0.9220, AP: 0.9362\n",
      "Epoch: 800, Loss_train: 0.8049, AUC: 0.9247, AP: 0.9374\n",
      "Epoch: 900, Loss_train: 0.8016, AUC: 0.9207, AP: 0.9347\n",
      "Epoch: 1000, Loss_train: 0.7936, AUC: 0.9197, AP: 0.9360\n",
      "Epoch: 1100, Loss_train: 0.7959, AUC: 0.9231, AP: 0.9384\n",
      "Epoch: 1200, Loss_train: 0.7908, AUC: 0.9260, AP: 0.9429\n",
      "Epoch: 1300, Loss_train: 0.7793, AUC: 0.9234, AP: 0.9427\n",
      "Epoch: 1400, Loss_train: 0.7816, AUC: 0.9217, AP: 0.9419\n",
      "Epoch: 1500, Loss_train: 0.7842, AUC: 0.9215, AP: 0.9408\n",
      "Epoch: 1600, Loss_train: 0.7816, AUC: 0.9220, AP: 0.9409\n",
      "Epoch: 1700, Loss_train: 0.7733, AUC: 0.9225, AP: 0.9405\n",
      "Epoch: 1800, Loss_train: 0.7672, AUC: 0.9255, AP: 0.9409\n",
      "Epoch: 1900, Loss_train: 0.7725, AUC: 0.9267, AP: 0.9421\n",
      "Epoch: 2000, Loss_train: 0.7784, AUC: 0.9256, AP: 0.9408\n"
     ]
    }
   ],
   "source": [
    "model = GAE(GCNEncoder(in_channels, out_channels))\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    " \n",
    "\n",
    "epochs = 2000\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_gae(model)\n",
    "    if epoch % 100 == 0:\n",
    "        auc, ap = test(model, test_data)\n",
    "        print('Epoch: {:03d}, Loss_train: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练VGAE："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss_train: 1.1616, AUC: 0.7363, AP: 0.7461\n",
      "Epoch: 200, Loss_train: 1.0253, AUC: 0.8482, AP: 0.8456\n",
      "Epoch: 300, Loss_train: 0.9404, AUC: 0.8730, AP: 0.8710\n",
      "Epoch: 400, Loss_train: 0.9141, AUC: 0.8971, AP: 0.9038\n",
      "Epoch: 500, Loss_train: 0.8851, AUC: 0.9088, AP: 0.9174\n",
      "Epoch: 600, Loss_train: 0.8822, AUC: 0.9111, AP: 0.9208\n",
      "Epoch: 700, Loss_train: 0.8721, AUC: 0.9168, AP: 0.9257\n",
      "Epoch: 800, Loss_train: 0.8667, AUC: 0.9179, AP: 0.9264\n",
      "Epoch: 900, Loss_train: 0.8668, AUC: 0.9163, AP: 0.9274\n",
      "Epoch: 1000, Loss_train: 0.8489, AUC: 0.9186, AP: 0.9292\n",
      "Epoch: 1100, Loss_train: 0.8485, AUC: 0.9212, AP: 0.9333\n",
      "Epoch: 1200, Loss_train: 0.8500, AUC: 0.9204, AP: 0.9324\n",
      "Epoch: 1300, Loss_train: 0.8466, AUC: 0.9253, AP: 0.9361\n",
      "Epoch: 1400, Loss_train: 0.8415, AUC: 0.9237, AP: 0.9346\n",
      "Epoch: 1500, Loss_train: 0.8425, AUC: 0.9252, AP: 0.9373\n",
      "Epoch: 1600, Loss_train: 0.8358, AUC: 0.9278, AP: 0.9397\n",
      "Epoch: 1700, Loss_train: 0.8368, AUC: 0.9247, AP: 0.9382\n",
      "Epoch: 1800, Loss_train: 0.8326, AUC: 0.9286, AP: 0.9412\n",
      "Epoch: 1900, Loss_train: 0.8192, AUC: 0.9298, AP: 0.9419\n",
      "Epoch: 2000, Loss_train: 0.8365, AUC: 0.9260, AP: 0.9395\n"
     ]
    }
   ],
   "source": [
    "model = VGAE(VariationalGCNEncoder(in_channels, out_channels))\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    " \n",
    "epochs = 2000\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss = train_vgae(model)\n",
    "    if epoch % 100 == 0:\n",
    "        auc, ap = test(model, test_data)\n",
    "        print('Epoch: {:03d}, Loss_train: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
