{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network Basic "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Neural Network (GNN) is a generalisation of Deep Neural Network (DNN) on graph-strucutred. In this session, we will explain common graph neural networks such as GCN and GAT, and how to use graph neural networks to accomplish node classification and graph classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GCN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 GCN in Matrix form "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please what you have learned in each layer of the GCN. Given a graph with an adjacency matrix $A$ and a node feature matrix $X$, if we use $W$ to denote the parameters in the GCN layer and $H$ to denote the features in the hidden layer, then the operation in a layer of GCN can be expressed as $H=\\text{ReLU}(AXW)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please import toolkits \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy.sparse as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"GCN layer，Refer to https://github.com/tkipf/pygcn\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    in_features : dim of input feature\n",
    "    out_features : output channel of feature (the same as out_channel in CNN)\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, with_bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features)) # Create weight matrix in shape (in_features, out_features)\n",
    "        if with_bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self): \n",
    "        \"\"\"Initializing your weight params through any distribution\"\"\"\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"Forward\"\"\"\n",
    "        # Your Code\n",
    "        if x.data.is_sparse:\n",
    "            # sparse matrix \n",
    "            support = torch.spmm(x, self.weight) # XW\n",
    "        else:\n",
    "            support = torch.mm(x, self.weight) # XW\n",
    "        output = torch.spmm(adj, support) # AXW\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias # AXW + b\n",
    "        else:\n",
    "            return output # AXW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    \"\"\" two layers GCN\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : input dim of feature\n",
    "    nhid : dim of hidden neuron\n",
    "    nclass : number of classes\n",
    "    dropout : dropout probability (less than 0.3)\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.nfeat = nfeat\n",
    "        self.hidden_sizes = [nhid]\n",
    "        self.nclass = nclass\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid, with_bias=with_bias)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass, with_bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        # The implementation of GCN is the same as CNN\n",
    "        # Your architecture should be input -> gc1 -> Act -> dropout -> gc2 -> softmax/logsoftmax\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"initializing \"\"\"\n",
    "        self.gc1.reset_parameters()\n",
    "        self.gc2.reset_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Classification using GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "dataset = Planetoid(root='./data', name='Cora') \n",
    "data = dataset[0]\n",
    "adj = to_scipy_sparse_matrix(data.edge_index)\n",
    "features = data.x\n",
    "labels = data.y   \n",
    "labels.max()\n",
    "adj.row\n",
    "adj.col\n",
    "adj.data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize： \n",
    "\n",
    "$A \\leftarrow A + I$\n",
    "\n",
    "$\\hat{A}= D^{-1/2}{A}D^{-1/2}$ \n",
    "\n",
    "Laplacian Matrix: \n",
    "$L = D^{-\\frac12}(D-A)D^{-\\frac12} = I - D^{-\\frac12}AD^{-\\frac12}$  \n",
    "\n",
    "$h=Lf=(D-A)f=Df-Af$ \n",
    "\n",
    "$h[i]=\\sum_{v_j \\in \\mathcal{N}(v_i)}(f[i]-f[j])$\n",
    "\n",
    "\n",
    "$A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"Standarize：A' = (D + I)^-1/2 * ( A + I ) * (D + I)^-1/2\n",
    "    \"\"\"\n",
    "    mx = mx + sp.eye(mx.shape[0])\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1/2).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    mx = mx.dot(r_mat_inv)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 2708)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_norm = normalize_adj(adj)\n",
    "adj_norm.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"transform scipy.sparse into torch 's sparse tensor\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    sparserow=torch.LongTensor(sparse_mx.row).unsqueeze(1)\n",
    "    sparsecol=torch.LongTensor(sparse_mx.col).unsqueeze(1)\n",
    "    sparseconcat=torch.cat((sparserow, sparsecol),1)\n",
    "    sparsedata=torch.FloatTensor(sparse_mx.data)\n",
    "    return torch.sparse.FloatTensor(sparseconcat.t(),sparsedata,torch.Size(sparse_mx.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_norm = sparse_mx_to_torch_sparse_tensor(adj_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implemetation with one example, good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 7])\n",
      "tensor([[-1.7591, -2.0253, -2.0189,  ..., -2.3352, -1.6869, -1.7229],\n",
      "        [-1.6986, -1.8106, -2.2891,  ..., -2.3748, -1.7933, -1.9212],\n",
      "        [-1.7677, -1.7952, -2.2800,  ..., -2.1975, -1.7906, -1.8656],\n",
      "        ...,\n",
      "        [-1.8508, -1.6366, -2.2955,  ..., -2.2564, -1.6579, -2.1074],\n",
      "        [-1.6509, -1.8511, -2.1399,  ..., -2.1514, -1.9642, -1.9625],\n",
      "        [-1.6734, -1.9029, -2.1107,  ..., -2.1453, -1.9678, -1.9103]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nclass = labels.max().item()+1\n",
    "model = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "output = model(features, adj_norm)\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 GCN in PyTorch Geometric \n",
    "Compare with matrix form GCN, adj is equivalent to edge_index and edge_weight while node features is called x.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\" Two Layers' GCN\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : dimension of input \n",
    "    nhid : dim of hidden neurons\n",
    "    nclass : number of classes ground truth\n",
    "    dropout : dropout probability\n",
    "    with_bias: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.5, with_bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(nfeat, nhid, bias=with_bias, activation=F.relu)\n",
    "        self.conv2 = GCNConv(nhid, nclass, bias=with_bias)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize\"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, data, features=None):\n",
    "        \"\"\"Forward Your code\"\"\"\n",
    "        x, edge_index = data.x, data.edge_index \n",
    "        x = F.relu(self.conv1(x, edge_index)) \n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9880, -1.9541, -1.9726,  ..., -1.9304, -1.9281, -1.8912],\n",
      "        [-2.2127, -1.8580, -1.9494,  ..., -1.9297, -1.7055, -2.0121],\n",
      "        [-2.1711, -1.8930, -1.9549,  ..., -1.8984, -1.8401, -1.8733],\n",
      "        ...,\n",
      "        [-2.1666, -1.8851, -2.0241,  ..., -1.7733, -1.8990, -1.9040],\n",
      "        [-1.9827, -1.9461, -1.9360,  ..., -1.9300, -1.9318, -1.9620],\n",
      "        [-1.9930, -1.9411, -1.9337,  ..., -1.9335, -1.9234, -1.9689]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "nclass = labels.max().item()+1\n",
    "model = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "output = model(data)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN Variant\n",
    "\n",
    "\\begin{equation}\n",
    "e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
    "\\end{equation}\n",
    "\n",
    "Masked Attention:\n",
    "softmax function:\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
    "\\end{equation}\n",
    "\n",
    "Now, we use the normalized attention coefficients to compute a linear combination of the features corresponding to them. These aggregated features will serve as the final output features for every node.\n",
    "\n",
    "\\begin{equation}\n",
    "h_i' = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "class GAT(nn.Module):\n",
    "    \"\"\" Two layers' GAT.\n",
    "    \n",
    "    Params\n",
    "    ----------\n",
    "    nfeat : dim of input features\n",
    "    nhid : dim of hidden neurons\n",
    "    nclass : output of classes\n",
    "    heads: number of head in attention mechanism\n",
    "    output_heads: output head\n",
    "    dropout : dropout probability\n",
    "    with_bias: with or without bias\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, heads=8, output_heads=1, dropout=0.5, with_bias=True):\n",
    "\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.conv1 = GATConv(\n",
    "            nfeat,\n",
    "            nhid,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.conv2 = GATConv(\n",
    "            nhid * heads,\n",
    "            nclass,\n",
    "            heads=output_heads,\n",
    "            concat=False,\n",
    "            dropout=dropout,\n",
    "            bias=with_bias)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize\n",
    "        \"\"\"\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9340, -2.2271, -1.9123,  ..., -1.8488, -1.9332, -2.0142],\n",
      "        [-2.1567, -2.1350, -2.0301,  ..., -1.8073, -1.7338, -2.0235],\n",
      "        [-2.0334, -1.8576, -1.8697,  ..., -2.0336, -1.8977, -2.1896],\n",
      "        ...,\n",
      "        [-1.6584, -1.9193, -1.8506,  ..., -2.0877, -1.7589, -2.5035],\n",
      "        [-1.9157, -1.9448, -1.8049,  ..., -2.0337, -1.8875, -1.9649],\n",
      "        [-2.0115, -2.0712, -1.8277,  ..., -2.0300, -1.7620, -1.8970]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "gat = GAT(nfeat=features.shape[1],\n",
    "      nhid=8, heads=8,\n",
    "      nclass=nclass)\n",
    "output = gat(data)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, lr=0.01, weight_decay=5e-4, epochs=200):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    labels = data.y\n",
    "    train_mask = data.train_mask\n",
    "    best_loss_val = 100\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # only training nodes will be used to calculate loss \n",
    "        loss = F.nll_loss(output[train_mask], labels[train_mask]) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch {}, training loss: {}'.format(i, loss.item()))\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    \"\"\"Evaluate GAT performance on test set.\n",
    "\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_mask = data.test_mask\n",
    "    labels = data.y \n",
    "    output = model(data) \n",
    "    loss_test = F.nll_loss(output[test_mask], labels[test_mask])\n",
    "    preds = output[test_mask].argmax(1) \n",
    "    acc_test = preds.eq(labels[test_mask]).cpu().numpy().mean() \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test))\n",
    "    return preds, output, acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 1.9520642757415771\n",
      "Epoch 10, training loss: 0.877902090549469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, training loss: 0.25019118189811707\n",
      "Epoch 30, training loss: 0.10044587403535843\n",
      "Epoch 40, training loss: 0.08083892613649368\n",
      "Epoch 50, training loss: 0.044564470648765564\n",
      "Epoch 60, training loss: 0.050442885607481\n",
      "Epoch 70, training loss: 0.043591082096099854\n",
      "Epoch 80, training loss: 0.0554530993103981\n",
      "Epoch 90, training loss: 0.05887354537844658\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nfeat=features.shape[1], nhid=16, nclass=nclass)\n",
    "device = 'cpu' # device ='cuda'\n",
    "model = model.to(device)\n",
    "data = data.to(device)\n",
    "train(model, data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.6722 accuracy= 0.7930\n"
     ]
    }
   ],
   "source": [
    "preds, output, acc = test(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification using GCN\n",
    "In Graph Classification, each label is related to the whole graph but not the embedding vector of each node. We use pooling and a linear layer to generate this label.\n",
    "\n",
    "## Dataset [Enzymes](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.TUDataset.html#torch_geometric.datasets.TUDataset)\n",
    "In ENZYMES dataset, there are 6 classes as ground truth, 18 continuous node features and three different kinds of node types as feature matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "dataset = TUDataset(root='./data/ENZYMES', name='ENZYMES', use_node_attr=True)\n",
    "dataset = dataset.shuffle()\n",
    "train_ratio = 0.8 \n",
    "test_ratio = 0.2\n",
    "train_dataset = dataset[: int(train_ratio*len(dataset))]\n",
    "test_dataset = dataset[-int(test_ratio*len(dataset)):]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 148], x=[38, 21], y=[1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \"\"\" 3 layers' GCN + Linar layers\"\"\"\n",
    "\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout=0.2):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.gc1 = GCNConv(nfeat, nhid)\n",
    "        self.gc2 = GCNConv(nhid, nhid)\n",
    "        self.gc3 = GCNConv(nhid, nhid)\n",
    "        self.lin = nn.Linear(nhid, nclass) # output of the whole graph\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.gc1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.gc2(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training) \n",
    "        x = self.gc3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch=data.batch) \n",
    "        x = self.lin(x) \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeat = dataset.num_node_features\n",
    "nclass = dataset.num_classes\n",
    "nhid = 64\n",
    "device = 'cpu'\n",
    "model = GCN(nfeat, nhid, nclass).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr=0.001, epochs=1000):\n",
    "    \"\"\"train loop\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_all = 0\n",
    "        for data in (train_loader): \n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_model = model(data)\n",
    "            loss = F.nll_loss(y_model, data.y)\n",
    "            loss.backward()\n",
    "            loss_all += loss.item() * data.num_graphs\n",
    "            optimizer.step()\n",
    "        loss_train = loss_all / len(train_loader.dataset) \n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch: {:03d}, Loss: {:.7f}'.format(epoch, loss_train))\n",
    "\n",
    "@torch.no_grad()            \n",
    "def test(model, loader):\n",
    "    \"\"\"test loop\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        _, pred = model(data).max(dim=1)\n",
    "        correct += float(pred.eq(data.y).sum().item())\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 2.7769059\n",
      "Epoch: 100, Loss: 1.5065527\n",
      "Epoch: 200, Loss: 1.2350971\n",
      "Epoch: 300, Loss: 0.9805408\n",
      "Epoch: 400, Loss: 0.8096992\n",
      "Epoch: 500, Loss: 0.6969201\n",
      "Epoch: 600, Loss: 0.5146528\n",
      "Epoch: 700, Loss: 0.4461468\n",
      "Epoch: 800, Loss: 0.3612646\n",
      "Epoch: 900, Loss: 0.3089069\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5666666666666667"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "39b3724457e4e3cbefcb2bd30301939350cec3d4d79128efb8d8fd313b7931f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
